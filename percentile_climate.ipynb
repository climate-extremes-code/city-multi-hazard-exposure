{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481625a1-ee7c-4b2c-ac99-9736b6ad33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script merges annual CMIP6 NetCDF files (downloaded from ESGF, one file per year)\n",
    "into a single continuous file along the time dimension for a given variable and model.\n",
    "Used here to create merged datasets for 2015–2030 and 2080–2100 periods.\n",
    "\"\"\"\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Command-line arguments\n",
    "parser = argparse.ArgumentParser(description=\"Merge yearly NetCDF files into one along the time dimension.\")\n",
    "parser.add_argument(\"--base_dir\", required=True, help=\"Base directory containing the NetCDF files\")\n",
    "parser.add_argument(\"--variable\", required=True, help=\"Variable name, e.g., pr, tas\")\n",
    "parser.add_argument(\"--model\", required=True, help=\"Climate model name\")\n",
    "parser.add_argument(\"--scenario\", default=\"ssp585\", help=\"Scenario name (default: ssp585)\")\n",
    "parser.add_argument(\"--member\", default=\"r1i1p1f1\", help=\"Ensemble member (default: r1i1p1f1)\")\n",
    "parser.add_argument(\"--grid\", default=\"gn\", help=\"Grid label (default: gn)\")\n",
    "parser.add_argument(\"--start\", type=int, required=True, help=\"Start year\")\n",
    "parser.add_argument(\"--end\", type=int, required=True, help=\"End year\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Build the list of NetCDF files for the given range of years\n",
    "nc_files = [\n",
    "    os.path.join(\n",
    "        args.base_dir,\n",
    "        args.model,\n",
    "        \"Daily\",\n",
    "        args.variable,\n",
    "        f\"{args.variable}_day_{args.model}_{args.scenario}_{args.member}_{args.grid}_{year}0101-{year}1231.nc\"\n",
    "    )\n",
    "    for year in range(args.start, args.end + 1)\n",
    "]\n",
    "\n",
    "# Load all files into a list of xarray Datasets with progress bar\n",
    "datasets = []\n",
    "for file in tqdm(nc_files, desc=\"Loading files\"):\n",
    "    datasets.append(xr.open_dataset(file))\n",
    "    tqdm.write(f\"Loaded {file}\")\n",
    "\n",
    "# Concatenate the datasets along the time dimension\n",
    "combined_dataset = xr.concat(datasets, dim='time')\n",
    "print(\"All files have been concatenated successfully.\")\n",
    "\n",
    "# Output file path\n",
    "output_dir = os.path.join(args.base_dir, args.model, \"Daily\", args.variable, \"merged\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(\n",
    "    output_dir,\n",
    "    f\"{args.variable}_day_{args.model}_{args.scenario}_{args.member}_{args.grid}_{args.start}0101-{args.end}1231.nc\"\n",
    ")\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_dataset.to_netcdf(output_path)\n",
    "print(f\"Merged file saved to {output_path}\")\n",
    "\n",
    "# Close datasets to free resources\n",
    "for ds in datasets:\n",
    "    ds.close()\n",
    "print(\"All datasets have been closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e529f-8502-4165-ac2a-56d19dfd0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter CMIP6 NetCDF climate model outputs (tas, tasmax, tasmin, pr) \n",
    "to include only grid cells corresponding to populated places from \n",
    "a Natural Earth shapefile. The script:\n",
    "\n",
    "1. Loads the shapefile of populated places and ensures WGS84 CRS.\n",
    "2. Opens each NetCDF, slices to the given time period (default 2080–2100).\n",
    "3. Identifies the nearest model grid cell to each city centroid (nearest-neighbor selection, no interpolation).\n",
    "4. Masks all other grid cells and keeps data only for these urban points.\n",
    "5. Saves the filtered dataset to a new NetCDF file.\n",
    "\n",
    "Note:\n",
    "- Uses nearest-neighbor selection to match shapefile centroids to model grid points.\n",
    "- Does not perform spatial averaging; results correspond to the single closest grid cell.\n",
    "\n",
    "Intended for extracting climate projections over urban locations.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# --- time slicing helper ---\n",
    "def time_slice(dataset, start=\"2080-01-01\", end=\"2100-12-31\"):\n",
    "    cal = dataset.time.encoding.get(\"calendar\", \"standard\")\n",
    "    # 360-day calendars don't have Dec 31\n",
    "    if cal == \"360_day\":\n",
    "        end = \"2100-12-30\"\n",
    "    return dataset.sel(time=slice(start, end))\n",
    "\n",
    "def process_nc_file(nc_file_path, shapefile_path, output_folder, start_date, end_date):\n",
    "    # 1) Load shapefile (Natural Earth populated places is WGS84 / lon-lat)\n",
    "    urban_areas = gpd.read_file(shapefile_path)\n",
    "    if urban_areas.empty:\n",
    "        raise ValueError(\"Shapefile is empty.\")\n",
    "    # Ensure CRS is WGS84\n",
    "    if urban_areas.crs is None or urban_areas.crs.to_epsg() != 4326:\n",
    "        urban_areas = urban_areas.to_crs(4326)\n",
    "\n",
    "    # 2) Open NetCDF and slice time (use_cftime handles non-standard calendars)\n",
    "    ds = xr.open_dataset(nc_file_path, use_cftime=True)\n",
    "    ds = time_slice(ds, start=start_date, end=end_date)\n",
    "\n",
    "    # 3) Pick the climate variable\n",
    "    candidates = [v for v in ds.data_vars if v in {\"tas\", \"tasmax\", \"tasmin\", \"pr\"}]\n",
    "    if not candidates:\n",
    "        print(f\"Skipping {nc_file_path}: no target variable found (tas/tasmax/tasmin/pr).\")\n",
    "        return\n",
    "    variable = candidates[0]\n",
    "    da = ds[variable]\n",
    "\n",
    "    # 4) Prepare grid for nearest-neighbor search\n",
    "    # normalize longitudes to [-180, 180] to match Natural Earth lon range\n",
    "    if \"lon\" not in ds.coords or \"lat\" not in ds.coords:\n",
    "        print(f\"Skipping {nc_file_path}: 'lat'/'lon' coords not found.\")\n",
    "        return\n",
    "\n",
    "    lon = ds[\"lon\"].values\n",
    "    lat = ds[\"lat\"].values\n",
    "\n",
    "    # Handle 1D vs 2D lon/lat\n",
    "    if lon.ndim == 1 and lat.ndim == 1:\n",
    "        # 1D coords -> build 2D mesh\n",
    "        lon2d, lat2d = np.meshgrid(lon, lat)\n",
    "    elif lon.ndim == 2 and lat.ndim == 2:\n",
    "        lon2d, lat2d = lon, lat\n",
    "    else:\n",
    "        print(f\"Skipping {nc_file_path}: unexpected lat/lon dimensions.\")\n",
    "        return\n",
    "\n",
    "    lon2d_norm = ((lon2d + 180) % 360) - 180  # shift to [-180, 180]\n",
    "\n",
    "    # 5) KD-tree over all grid points\n",
    "    grid_points = np.column_stack([lon2d_norm.ravel(), lat2d.ravel()])\n",
    "    tree = cKDTree(grid_points)\n",
    "\n",
    "    # 6) Find nearest grid index for each urban centroid\n",
    "    # (centroid in degrees is OK for picking nearest cell on a regular grid)\n",
    "    centroids = urban_areas.geometry.centroid\n",
    "    mask = np.zeros(lon2d.shape, dtype=bool)\n",
    "    for pt in centroids:\n",
    "        if pt.is_empty:\n",
    "            continue\n",
    "        # (x=lon, y=lat)\n",
    "        lon_pt = float(pt.x)\n",
    "        lat_pt = float(pt.y)\n",
    "        _, idx = tree.query((lon_pt, lat_pt), k=1)\n",
    "        iy, ix = np.unravel_index(idx, lon2d.shape)\n",
    "        mask[iy, ix] = True\n",
    "\n",
    "    # 7) Apply mask: keep only those grid cells (all time)\n",
    "    mask_da = xr.DataArray(\n",
    "        mask,\n",
    "        dims=(\"lat\", \"lon\"),\n",
    "        coords={\"lat\": ds[\"lat\"], \"lon\": ds[\"lon\"]},\n",
    "    )\n",
    "    filtered = da.where(mask_da, drop=True)\n",
    "\n",
    "    # 8) Save output\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    base = os.path.basename(nc_file_path).replace(\".nc\", \"\")\n",
    "    out_path = os.path.join(output_folder, f\"{base}_filtered_HW_Drought.nc\")\n",
    "    filtered.to_netcdf(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Filter NetCDFs to urban cells from a Natural Earth shapefile, 2080–2100.\")\n",
    "    ap.add_argument(\"--shapefile\", required=True, help=\"Path to Natural Earth populated places shapefile (e.g., ne_10m_populated_places.shp)\")\n",
    "    ap.add_argument(\"--output_dir\", required=True, help=\"Directory to write filtered NetCDFs\")\n",
    "    ap.add_argument(\"--start\", default=\"2080-01-01\", help=\"Start date (default: 2080-01-01)\")\n",
    "    ap.add_argument(\"--end\", default=\"2100-12-31\", help=\"End date (default: 2100-12-31; 360_day calendars use 2100-12-30)\")\n",
    "    ap.add_argument(\"nc_files\", nargs=\"+\", help=\"One or more NetCDF file paths\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    for path in args.nc_files:\n",
    "        try:\n",
    "            process_nc_file(path, args.shapefile, args.output_dir, args.start, args.end)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {path} -> {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb14c2-b5c3-4ec3-b715-5f61fce19fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute multi-model 75th percentile daily climate values (Step 1 + Step 2 of method).\n",
    "\n",
    "Method summary:\n",
    "1. For each GCM file, compute the per-model daily mean (averaged over all years in the target period).\n",
    "2. For each day-of-year and grid cell, compute the 75th percentile across all models.\n",
    "\n",
    "Assumptions:\n",
    "- Input files are per-model daily averages for the selected period (e.g., 2080–2100).\n",
    "- Each file contains only one climate variable of interest ('tas', 'tasmax', 'tasmin', 'pr').\n",
    "- All models share the same spatial grid.\n",
    "- If 'day_of_year' is not in the file, a 'time' column must exist to derive it.\n",
    "- Leap days (day 366) are included if present; adjust preprocessing if not desired.\n",
    "\n",
    "Example usage:\n",
    "    python compute_75th_percentile.py \\\n",
    "        --input_dir \"./data/Daily/2080-2100/averages\" \\\n",
    "        --output_file \"./output/75th_percentile_values-daily-2080-2100_HW_drought.csv\"\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# ------------------------\n",
    "# Parse arguments\n",
    "# ------------------------\n",
    "parser = argparse.ArgumentParser(description=\"Compute multi-model 75th percentile daily climate values.\")\n",
    "parser.add_argument(\"--input_dir\", required=True, help=\"Directory containing per-model daily averages CSV files\")\n",
    "parser.add_argument(\"--output_file\", required=True, help=\"Path to save the final merged CSV file\")\n",
    "parser.add_argument(\"--variables\", nargs=\"+\", default=['tas', 'tasmax', 'tasmin', 'pr'],\n",
    "                    help=\"List of climate variables to process\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "directory_path = args.input_dir\n",
    "variables = args.variables\n",
    "\n",
    "# Collect per-model daily means\n",
    "per_model_means = {var: [] for var in variables}\n",
    "error_log = []\n",
    "\n",
    "def detect_var(df):\n",
    "    \"\"\"Identify which climate variable is in the DataFrame.\"\"\"\n",
    "    for v in variables:\n",
    "        if v in df.columns:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "# ------------------------\n",
    "# Step 1: Per-model daily mean\n",
    "# ------------------------\n",
    "file_paths = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    print(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure day_of_year exists\n",
    "        if 'day_of_year' not in df.columns:\n",
    "            if 'time' in df.columns:\n",
    "                df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "                df = df.dropna(subset=['time'])\n",
    "                df['day_of_year'] = df['time'].dt.dayofyear\n",
    "            else:\n",
    "                raise ValueError(\"No 'day_of_year' or 'time' column found.\")\n",
    "\n",
    "        # Identify variable\n",
    "        var = detect_var(df)\n",
    "        if var is None:\n",
    "            raise ValueError(\"No valid variable column found (tas/tasmax/tasmin/pr).\")\n",
    "\n",
    "        # Compute per-model daily mean over years\n",
    "        daily_mean = (\n",
    "            df.groupby(['lon', 'lat', 'day_of_year'])[var]\n",
    "              .mean()\n",
    "              .reset_index()\n",
    "        )\n",
    "        per_model_means[var].append(daily_mean)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log.append(f\"Error processing {file_path}: {e}\")\n",
    "        print(error_log[-1])\n",
    "\n",
    "# ------------------------\n",
    "# Step 2: 75th percentile across models\n",
    "# ------------------------\n",
    "final_results = {}\n",
    "for var, parts in per_model_means.items():\n",
    "    if parts:\n",
    "        all_models = pd.concat(parts, ignore_index=True)\n",
    "        p75 = (\n",
    "            all_models\n",
    "            .groupby(['lon', 'lat', 'day_of_year'])[var]\n",
    "            .quantile(0.75)\n",
    "            .reset_index()\n",
    "        )\n",
    "        final_results[var] = p75\n",
    "\n",
    "if not final_results:\n",
    "    raise RuntimeError(\"No results computed. Check input files.\")\n",
    "\n",
    "# ------------------------\n",
    "# Merge all variables\n",
    "# ------------------------\n",
    "print(\"Merging results...\")\n",
    "first_var = next(iter(final_results))\n",
    "merged_df = final_results[first_var].rename(columns={first_var: f'75th_{first_var}'})\n",
    "\n",
    "for var in variables:\n",
    "    if var in final_results and var != first_var:\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            final_results[var].rename(columns={var: f'75th_{var}'}),\n",
    "            on=['lon', 'lat', 'day_of_year'],\n",
    "            how='outer'\n",
    "        )\n",
    "\n",
    "# ------------------------\n",
    "# Save results\n",
    "# ------------------------\n",
    "os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n",
    "merged_df.to_csv(args.output_file, index=False)\n",
    "print(f\"Saved final 75th percentile file to: {args.output_file}\")\n",
    "\n",
    "# Save error log if needed\n",
    "if error_log:\n",
    "    error_log_file = os.path.join(os.path.dirname(args.output_file), \"error_log.csv\")\n",
    "    with open(error_log_file, 'w') as f:\n",
    "        for line in error_log:\n",
    "            f.write(line + '\\n')\n",
    "    print(f\"Error log saved to {error_log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
