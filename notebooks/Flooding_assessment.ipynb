{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4dd79-32a3-4a30-9205-060c8a72f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flooding methodology\n",
    "---------------------------------------------------------\n",
    "This script implements a continent-by-continent flooding assessment consistent with our\n",
    "stated method.\n",
    "\n",
    "Upstream (not in this script):\n",
    "- GRDC daily discharge screening (1920–2024) with ≥9 continuous years and <5% missing.\n",
    "- GEV fitting on annual maxima to get historical 10‑year return levels (Q10) + 95% CIs\n",
    "  per station. The resulting table is passed in via --historical-q10.\n",
    "\n",
    "In this script:\n",
    "1) Match each GRDC station (geo_x, geo_y) to the **nearest climate grid cell** using\n",
    "   a Haversine match, for both historical and future monthly files.\n",
    "2) Convert units (K→°C; kg m⁻² s⁻¹→mm/month), then compute **snowpack** and **PET**\n",
    "   exactly as defined:\n",
    "   - Snowpack proxy = monthly precip when **tasmin < 0 °C**, else 0 (binary by month).\n",
    "   - PET = Thornthwaite monthly PET using **mean monthly T**, **gauge latitude**,\n",
    "     **daylength by month**, and **days in month**:\n",
    "       PET_m = 16 * (10*T/I)^a * (L/12) * (N/30), T>0, I>0.\n",
    "3) Aggregate monthly variables to **period means per station**.\n",
    "4) **Train** (mode=train) regressors per continent on **log(Q10)** with optional\n",
    "   interactions; pick best by test RMSE (RandomForest / GradientBoosting / optional XGBoost).\n",
    "   Save model + feature list per continent.\n",
    "5) **Predict** (mode=predict) future Q10 (same features), and compute **CF = Q10_future/Q10_hist**.\n",
    "\n",
    "Inputs (CSV schemas expected; column names configurable via flags):\n",
    "- --historical-q10 : station metadata + historical Q10\n",
    "    required cols: Station_ID, continent, basin_area_km2, Q10_hist, geo_x, geo_y\n",
    "- --historical-climate / --future-climate : monthly gridded climate\n",
    "    required cols: lat, lon, month(1..12), pr, tas, tasmax, tasmin\n",
    "\n",
    "Outputs (under --output-dir):\n",
    "- models/<Continent>.pkl                         (best model)\n",
    "- features_<Continent>.json                      (feature list used)\n",
    "- summary_best_by_continent.csv                  (RMSE/R²/AdjR²)\n",
    "- Africa_future_q10_predictions.csv (etc.)       (Station_ID, Q10_hist, Q10_future_pred, CF)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    _HAVE_XGB = True\n",
    "except Exception:\n",
    "    _HAVE_XGB = False\n",
    "\n",
    "# ---------------------------\n",
    "# Constants & small helpers\n",
    "# ---------------------------\n",
    "_DAYS_IN_MONTH = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, cols: Sequence[str], label: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{label} missing columns: {missing}\")\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)\n",
    "    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad)*np.cos(lat2_rad)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R*c\n",
    "\n",
    "\n",
    "def match_stations_to_grid(stations: pd.DataFrame, climate_grid: pd.DataFrame,\n",
    "                           id_col: str, xcol: str, ycol: str,\n",
    "                           lat_col: str = \"lat\", lon_col: str = \"lon\") -> pd.DataFrame:\n",
    "    \"\"\"Return mapping of each station to the nearest grid point (lat, lon).\"\"\"\n",
    "    _ensure_columns(stations, [id_col, xcol, ycol], \"stations\")\n",
    "    _ensure_columns(climate_grid, [lat_col, lon_col], \"climate_grid\")\n",
    "\n",
    "    # unique grid points to speed up search\n",
    "    grid_pts = climate_grid[[lat_col, lon_col]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    rows = []\n",
    "    for sid, gx, gy in tqdm(stations[[id_col, xcol, ycol]].itertuples(index=False), total=len(stations)):\n",
    "        dists = haversine(gy, gx, grid_pts[lat_col].values, grid_pts[lon_col].values)\n",
    "        idx = int(np.argmin(dists))\n",
    "        rows.append((sid, grid_pts.at[idx, lat_col], grid_pts.at[idx, lon_col]))\n",
    "    mapping = pd.DataFrame(rows, columns=[id_col, lat_col, lon_col])\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Unit conversions\n",
    "# ---------------------------\n",
    "\n",
    "def to_celsius_inplace(df: pd.DataFrame, cols: Sequence[str]) -> None:\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype(float) - 273.15\n",
    "\n",
    "\n",
    "def pr_to_mm_month_inplace(df: pd.DataFrame, pr_col: str, month_col: str) -> None:\n",
    "    df[month_col] = df[month_col].astype(int)\n",
    "    if not df[month_col].between(1, 12).all():\n",
    "        bad = df.loc[~df[month_col].between(1, 12), month_col].unique().tolist()\n",
    "        raise ValueError(f\"Month must be 1..12, found: {bad}\")\n",
    "    seconds_per_day = 86400.0\n",
    "    days = df[month_col].map(_DAYS_IN_MONTH).astype(float)\n",
    "    df[\"pr_mm_month\"] = df[pr_col].astype(float) * seconds_per_day * days\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Thornthwaite PET\n",
    "# ---------------------------\n",
    "\n",
    "def _monthly_day_of_year_midpoint(month: int) -> int:\n",
    "    return int(round(sum(_DAYS_IN_MONTH[m] for m in range(1, month)) + _DAYS_IN_MONTH[month]/2.0))\n",
    "\n",
    "\n",
    "def _daylength_hours(latitude_deg: float, month: int) -> float:\n",
    "    lat = float(latitude_deg)\n",
    "    if not (-90.0 <= lat <= 90.0):\n",
    "        raise ValueError(f\"Latitude out of bounds [-90,90]: {lat}\")\n",
    "    n = _monthly_day_of_year_midpoint(month)\n",
    "    decl_deg = 23.45 * np.sin(np.deg2rad((360.0/365.0)*(284 + n)))\n",
    "    phi = np.deg2rad(lat)\n",
    "    decl = np.deg2rad(decl_deg)\n",
    "    arg = np.clip(-np.tan(phi) * np.tan(decl), -1.0, 1.0)\n",
    "    sha = np.arccos(arg)\n",
    "    return float((24.0/np.pi) * sha)\n",
    "\n",
    "\n",
    "def _thornthwaite_I(monthly_tas_C: pd.Series) -> float:\n",
    "    tpos = monthly_tas_C[monthly_tas_C > 0.0]\n",
    "    if tpos.empty:\n",
    "        return 0.0\n",
    "    return float(np.sum((tpos/5.0)**1.514))\n",
    "\n",
    "\n",
    "def _thornthwaite_a(I: float) -> float:\n",
    "    return (6.75e-7 * I**3) - (7.71e-5 * I**2) + (1.79e-2 * I) + 0.49\n",
    "\n",
    "\n",
    "def _pet_month(T_C: float, I: float, lat_deg: float, month: int) -> float:\n",
    "    if (T_C <= 0.0) or (I <= 0.0):\n",
    "        return 0.0\n",
    "    a = _thornthwaite_a(I)\n",
    "    L = _daylength_hours(lat_deg, int(month))\n",
    "    N = _DAYS_IN_MONTH[int(month)]\n",
    "    return float(16.0 * ((10.0*T_C/I)**a) * (L/12.0) * (N/30.0))\n",
    "\n",
    "\n",
    "def add_pet_snowpack_monthly(df: pd.DataFrame,\n",
    "                             id_col: str,\n",
    "                             lat_col: str,\n",
    "                             month_col: str,\n",
    "                             tas_col: str,\n",
    "                             tasmin_col: str,\n",
    "                             pr_col_mm_month: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute I per station, PET per row, and snowpack per row; return a copy with columns added.\"\"\"\n",
    "    df = df.copy()\n",
    "    _ensure_columns(df, [id_col, lat_col, month_col, tas_col, tasmin_col, pr_col_mm_month], \"monthly climate\")\n",
    "\n",
    "    # Heat index I per station using mean monthly tas\n",
    "    t_monthly = df.groupby([id_col, month_col], as_index=False)[tas_col].mean()\n",
    "    I_per_id = (\n",
    "        t_monthly.groupby(id_col, as_index=False)\n",
    "        .agg(I=(tas_col, lambda s: _thornthwaite_I(s)))\n",
    "    )\n",
    "    df = df.merge(I_per_id, on=id_col, how=\"left\")\n",
    "    df[\"I\"] = df[\"I\"].fillna(0.0)\n",
    "\n",
    "    # PET\n",
    "    df[\"PET_mm\"] = df.apply(lambda r: _pet_month(float(r[tas_col]), float(r[\"I\"]), float(r[lat_col]), int(r[month_col])), axis=1)\n",
    "\n",
    "    # Snowpack\n",
    "    df[\"snow_pack_mm\"] = np.where(df[tasmin_col] < 0.0, df[pr_col_mm_month], 0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_period_means(df: pd.DataFrame, id_col: str) -> pd.DataFrame:\n",
    "    agg = df.groupby(id_col).agg({\n",
    "        \"pr_mm_month\": \"mean\",\n",
    "        \"tas_C\": \"mean\",\n",
    "        \"tasmax_C\": \"mean\",\n",
    "        \"tasmin_C\": \"mean\",\n",
    "        \"PET_mm\": \"mean\",\n",
    "        \"snow_pack_mm\": \"mean\",\n",
    "    }).rename(columns={\n",
    "        \"pr_mm_month\": \"pr_mm_month_mean\",\n",
    "        \"tas_C\": \"tas_C_mean\",\n",
    "        \"tasmax_C\": \"tasmax_C_mean\",\n",
    "        \"tasmin_C\": \"tasmin_C_mean\",\n",
    "        \"PET_mm\": \"PET_mm_mean\",\n",
    "        \"snow_pack_mm\": \"snow_pack_mm_mean\",\n",
    "    }).reset_index()\n",
    "    return agg\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Feature matrix\n",
    "# ---------------------------\n",
    "\n",
    "def build_features(agg_df: pd.DataFrame, stations_df: pd.DataFrame, id_col: str,\n",
    "                   min_area_km2: float,\n",
    "                   interactions: List[Tuple[str, str]]) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    base_feats = [\n",
    "        \"pr_mm_month_mean\",\n",
    "        \"tas_C_mean\",\n",
    "        \"tasmax_C_mean\",\n",
    "        \"tasmin_C_mean\",\n",
    "        \"PET_mm_mean\",\n",
    "        \"snow_pack_mm_mean\",\n",
    "        \"basin_area_km2\",\n",
    "    ]\n",
    "    need_cols = [id_col, \"continent\", \"basin_area_km2\", \"Q10_hist\"]\n",
    "    _ensure_columns(stations_df, need_cols, \"historical Q10 table\")\n",
    "\n",
    "    df = agg_df.merge(stations_df[[id_col, \"continent\", \"basin_area_km2\", \"Q10_hist\"]], on=id_col, how=\"inner\")\n",
    "    df = df[df[\"basin_area_km2\"] >= float(min_area_km2)].copy()\n",
    "\n",
    "    # interactions\n",
    "    feat_cols = base_feats.copy()\n",
    "    for a, b in interactions:\n",
    "        if a not in df.columns or b not in df.columns:\n",
    "            raise ValueError(f\"Missing columns for interaction {a} x {b}\")\n",
    "        name = f\"{a}__x__{b}\"\n",
    "        df[name] = df[a].astype(float) * df[b].astype(float)\n",
    "        feat_cols.append(name)\n",
    "\n",
    "    return df, feat_cols\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Modeling (per continent)\n",
    "# ---------------------------\n",
    "\n",
    "def fit_models_per_continent(df_all: pd.DataFrame, feature_cols: List[str], random_state: int = 42):\n",
    "    continents = sorted(df_all[\"continent\"].dropna().unique().tolist())\n",
    "    best_models: Dict[str, dict] = {}\n",
    "    rows = []\n",
    "\n",
    "    for cont in continents:\n",
    "        d = df_all[df_all[\"continent\"] == cont].dropna(subset=[\"Q10_hist\"]).copy()\n",
    "        if d.empty:\n",
    "            continue\n",
    "        y = np.log(d[\"Q10_hist\"].astype(float))\n",
    "        X = d[feature_cols].astype(float)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)\n",
    "\n",
    "        models = []\n",
    "        rf = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "        rf_grid = {\"n_estimators\": [300, 600, 900], \"max_depth\": [None, 12, 24],\n",
    "                   \"min_samples_split\": [2, 5, 10], \"min_samples_leaf\": [1, 2, 4]}\n",
    "        models.append((\"RandomForest\", rf, rf_grid))\n",
    "\n",
    "        gb = GradientBoostingRegressor(random_state=random_state)\n",
    "        gb_grid = {\"n_estimators\": [300, 600, 900], \"learning_rate\": [0.03, 0.05, 0.1], \"max_depth\": [2, 3, 4], \"subsample\": [0.7, 0.9, 1.0]}\n",
    "        models.append((\"GradientBoosting\", gb, gb_grid))\n",
    "\n",
    "        if _HAVE_XGB:\n",
    "            xgb = XGBRegressor(random_state=random_state, tree_method=\"hist\")\n",
    "            xgb_grid = {\"n_estimators\": [400, 800, 1200], \"max_depth\": [4, 6, 8], \"learning_rate\": [0.03, 0.06, 0.1], \"subsample\": [0.7, 0.9, 1.0], \"colsample_bytree\": [0.6, 0.8, 1.0]}\n",
    "            models.append((\"XGBoost\", xgb, xgb_grid))\n",
    "\n",
    "        best = None\n",
    "        for name, est, grid in models:\n",
    "            cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "            n_iter = min(25, sum(len(v) for v in grid.values()))\n",
    "            search = RandomizedSearchCV(est, grid, n_iter=n_iter, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, random_state=random_state)\n",
    "            search.fit(X_train, y_train)\n",
    "            pred = search.predict(X_test)\n",
    "            rmse = float(math.sqrt(mean_squared_error(y_test, pred)))\n",
    "            r2 = float(r2_score(y_test, pred))\n",
    "            n = len(y_test); p = X_test.shape[1]\n",
    "            adjr2 = float(1 - (1 - r2) * (n - 1) / max(1, (n - p - 1)))\n",
    "            rows.append({\"continent\": cont, \"model\": name, \"test_RMSE_logQ10\": rmse, \"test_R2\": r2, \"test_AdjR2\": adjr2, \"features\": \",\".join(feature_cols)})\n",
    "            if (best is None) or (rmse < best[\"rmse\"]):\n",
    "                best = {\"name\": name, \"estimator\": search.best_estimator_, \"rmse\": rmse, \"r2\": r2, \"adjr2\": adjr2}\n",
    "        best_models[cont] = best\n",
    "\n",
    "    summary = pd.DataFrame(rows)\n",
    "    best_by = summary.sort_values([\"continent\", \"test_RMSE_logQ10\"]).groupby(\"continent\").head(1).reset_index(drop=True)\n",
    "    best_by[\"chosen_model\"] = best_by[\"model\"]\n",
    "    return best_models, best_by, summary\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Orchestration\n",
    "# ---------------------------\n",
    "\n",
    "def load_station_table(path: str, id_col: str, xcol: str, ycol: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    need = [id_col, \"continent\", \"basin_area_km2\", \"Q10_hist\", xcol, ycol]\n",
    "    _ensure_columns(df, need, \"historical-q10\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_monthly(climate_csv: str, mapping: pd.DataFrame, id_col: str,\n",
    "                    lat_col: str, lon_col: str, month_col: str,\n",
    "                    temps_already_celsius: bool,\n",
    "                    pr_already_mm_month: bool) -> pd.DataFrame:\n",
    "    \"\"\"Join climate monthly grid to stations via (lat,lon), convert units, compute PET/snowpack.\"\"\"\n",
    "    clim = pd.read_csv(climate_csv)\n",
    "    _ensure_columns(clim, [lat_col, lon_col, month_col, \"pr\", \"tas\", \"tasmax\", \"tasmin\"], \"monthly climate\")\n",
    "\n",
    "    # Convert units\n",
    "    if not temps_already_celsius:\n",
    "        to_celsius_inplace(clim, [\"tas\", \"tasmax\", \"tasmin\"])\n",
    "        clim.rename(columns={\"tas\":\"tas_C\", \"tasmax\":\"tasmax_C\", \"tasmin\":\"tasmin_C\"}, inplace=True)\n",
    "    else:\n",
    "        clim.rename(columns={\"tas\":\"tas_C\", \"tasmax\":\"tasmax_C\", \"tasmin\":\"tasmin_C\"}, inplace=True)\n",
    "\n",
    "    if not pr_already_mm_month:\n",
    "        pr_to_mm_month_inplace(clim, pr_col=\"pr\", month_col=month_col)\n",
    "    else:\n",
    "        clim.rename(columns={\"pr\":\"pr_mm_month\"}, inplace=True)\n",
    "\n",
    "    # Attach station ids via nearest (lat,lon)\n",
    "    m = mapping.merge(clim, on=[lat_col, lon_col], how=\"left\")\n",
    "    _ensure_columns(m, [id_col, month_col, \"tas_C\", \"tasmin_C\", \"pr_mm_month\"], \"station-climate join\")\n",
    "\n",
    "    # Compute PET & snowpack using station latitude and monthly tas\n",
    "    m.rename(columns={\"geo_y\": \"station_lat\"}, inplace=True) if \"geo_y\" in m.columns else None\n",
    "    lat_for_pet = \"station_lat\" if \"station_lat\" in m.columns else lat_col\n",
    "\n",
    "    enriched = add_pet_snowpack_monthly(\n",
    "        m,\n",
    "        id_col=id_col,\n",
    "        lat_col=lat_for_pet,\n",
    "        month_col=month_col,\n",
    "        tas_col=\"tas_C\",\n",
    "        tasmin_col=\"tasmin_C\",\n",
    "        pr_col_mm_month=\"pr_mm_month\",\n",
    "    )\n",
    "\n",
    "    return enriched[[id_col, month_col, \"tas_C\", \"tasmax_C\", \"tasmin_C\", \"pr_mm_month\", \"PET_mm\", \"snow_pack_mm\"]]\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> int:\n",
    "    ap = argparse.ArgumentParser(description=\"Continent-by-continent flooding pipeline (training & projections)\")\n",
    "    ap.add_argument(\"mode\", choices=[\"train\", \"predict\"], help=\"train: fit models on historical; predict: apply models to future\")\n",
    "    ap.add_argument(\"--historical-q10\", required=True, help=\"CSV with Station_ID, continent, basin_area_km2, Q10_hist, geo_x, geo_y\")\n",
    "    ap.add_argument(\"--historical-climate\", help=\"Monthly climate CSV for baseline period (required for train)\")\n",
    "    ap.add_argument(\"--future-climate\", help=\"Monthly climate CSV for future period (required for predict)\")\n",
    "    ap.add_argument(\"--continent\", default=None, help=\"Optional filter to a single continent (e.g., Africa)\")\n",
    "\n",
    "    # Column names to match your files\n",
    "    ap.add_argument(\"--id-col\", default=\"Station_ID\")\n",
    "    ap.add_argument(\"--xcol\", default=\"geo_x\")\n",
    "    ap.add_argument(\"--ycol\", default=\"geo_y\")\n",
    "    ap.add_argument(\"--lat-col\", default=\"lat\")\n",
    "    ap.add_argument(\"--lon-col\", default=\"lon\")\n",
    "    ap.add_argument(\"--month-col\", default=\"month\")\n",
    "\n",
    "    ap.add_argument(\"--temps-already-celsius\", action=\"store_true\")\n",
    "    ap.add_argument(\"--pr-already-mm-month\", action=\"store_true\")\n",
    "    ap.add_argument(\"--min-area-km2\", type=float, default=500.0)\n",
    "    ap.add_argument(\"--output-dir\", required=True)\n",
    "    ap.add_argument(\"--random-state\", type=int, default=42)\n",
    "    ap.add_argument(\"--interactions-json\", default=None, help=\"JSON string or path: {\\\"Africa\\\":[[\\\"pr_mm_month_mean\\\",\\\"tas_C_mean\\\"]], ...}\")\n",
    "\n",
    "    args = ap.parse_args(argv)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    models_dir = os.path.join(args.output_dir, \"models\"); os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    # Load stations/Q10 and optional continent filter\n",
    "    stations = load_station_table(args.historical_q10, args.id_col, args.xcol, args.ycol)\n",
    "    if args.continent:\n",
    "        stations = stations[stations[\"continent\"] == args.continent].copy()\n",
    "        if stations.empty:\n",
    "            raise SystemExit(f\"No stations for continent={args.continent}\")\n",
    "\n",
    "    # Build interactions per continent\n",
    "    per_cont_interactions: Dict[str, List[Tuple[str, str]]] = {}\n",
    "    if args.interactions_json:\n",
    "        raw = args.interactions_json\n",
    "        if os.path.exists(raw):\n",
    "            with open(raw, \"r\", encoding=\"utf-8\") as f: raw = f.read()\n",
    "        js = json.loads(raw)\n",
    "        for k, v in js.items():\n",
    "            per_cont_interactions[k] = [tuple(p) for p in v]\n",
    "\n",
    "    # TRAIN MODE\n",
    "    if args.mode == \"train\":\n",
    "        if not args.historical_climate:\n",
    "            raise SystemExit(\"--historical-climate is required in train mode\")\n",
    "        # Collocation mapping using the historical grid\n",
    "        hist_grid = pd.read_csv(args.historical_climate, usecols=[args.lat_col, args.lon_col]).drop_duplicates()\n",
    "        mapping = match_stations_to_grid(stations, hist_grid, args.id_col, args.xcol, args.ycol, args.lat_col, args.lon_col)\n",
    "        # Attach station latitude (for PET wording correctness)\n",
    "        mapping = mapping.merge(stations[[args.id_col, args.ycol]], on=args.id_col, how=\"left\").rename(columns={args.ycol: \"geo_y\"})\n",
    "\n",
    "        hist_monthly = prepare_monthly(args.historical_climate, mapping, args.id_col, args.lat_col, args.lon_col, args.month_col,\n",
    "                                       args.temps_already_celsius, args.pr_already_mm_month)\n",
    "        hist_agg = aggregate_period_means(hist_monthly, args.id_col)\n",
    "\n",
    "        # Build features per continent and fit models\n",
    "        best_models = {}\n",
    "        best_rows = []\n",
    "        all_rows = []\n",
    "        for cont in sorted(stations[\"continent\"].dropna().unique()):\n",
    "            inters = per_cont_interactions.get(cont, [])\n",
    "            merged, feat_cols = build_features(hist_agg, stations[stations[\"continent\"] == cont], args.id_col, args.min_area_km2, inters)\n",
    "            if merged.empty:\n",
    "                continue\n",
    "            models, best_by, summary = fit_models_per_continent(merged, feat_cols, random_state=args.random_state)\n",
    "            if cont in models and models[cont] is not None:\n",
    "                best_models[cont] = models[cont]\n",
    "                # save model + features\n",
    "                with open(os.path.join(models_dir, f\"{cont}.pkl\"), \"wb\") as f:\n",
    "                    pickle.dump(models[cont][\"estimator\"], f)\n",
    "                with open(os.path.join(models_dir, f\"features_{cont}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(feat_cols, f, ensure_ascii=False, indent=2)\n",
    "            best_rows.append(best_by)\n",
    "            all_rows.append(summary)\n",
    "\n",
    "        if best_rows:\n",
    "            pd.concat(best_rows, ignore_index=True).to_csv(os.path.join(args.output_dir, \"summary_best_by_continent.csv\"), index=False)\n",
    "        if all_rows:\n",
    "            pd.concat(all_rows, ignore_index=True).to_csv(os.path.join(args.output_dir, \"summary_all_models_by_continent.csv\"), index=False)\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "    # PREDICT MODE\n",
    "    else:\n",
    "        if not args.future_climate:\n",
    "            raise SystemExit(\"--future-climate is required in predict mode\")\n",
    "        # Build mapping from the FUTURE grid (so we pick the nearest available point in that file)\n",
    "        fut_grid = pd.read_csv(args.future_climate, usecols=[args.lat_col, args.lon_col]).drop_duplicates()\n",
    "        mapping = match_stations_to_grid(stations, fut_grid, args.id_col, args.xcol, args.ycol, args.lat_col, args.lon_col)\n",
    "        mapping = mapping.merge(stations[[args.id_col, args.ycol]], on=args.id_col, how=\"left\").rename(columns={args.ycol: \"geo_y\"})\n",
    "\n",
    "        fut_monthly = prepare_monthly(args.future_climate, mapping, args.id_col, args.lat_col, args.lon_col, args.month_col,\n",
    "                                      args.temps_already_celsius, args.pr_already_mm_month)\n",
    "        fut_agg = aggregate_period_means(fut_monthly, args.id_col)\n",
    "\n",
    "        continents = sorted(stations[\"continent\"].dropna().unique().tolist())\n",
    "        for cont in continents:\n",
    "            model_path = os.path.join(models_dir, f\"{cont}.pkl\")\n",
    "            feats_path = os.path.join(models_dir, f\"features_{cont}.json\")\n",
    "            if not (os.path.exists(model_path) and os.path.exists(feats_path)):\n",
    "                print(f\"[warn] Missing model or features for {cont}; skipping predictions.\")\n",
    "                continue\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                est = pickle.load(f)\n",
    "            with open(feats_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                feat_cols = json.load(f)\n",
    "\n",
    "            inters = per_cont_interactions.get(cont, [])  # same interactions as training\n",
    "            # For predictions we still need basin_area_km2 & Q10_hist to compute CF\n",
    "            merged_fut, _ = build_features(fut_agg, stations[stations[\"continent\"] == cont], args.id_col, args.min_area_km2, inters)\n",
    "            if merged_fut.empty:\n",
    "                continue\n",
    "\n",
    "            Xf = merged_fut[feat_cols].astype(float)\n",
    "            logQ10_future = est.predict(Xf)\n",
    "            Q10_future = np.exp(logQ10_future)\n",
    "            out = merged_fut[[args.id_col]].copy()\n",
    "            out[\"Q10_hist\"] = merged_fut[\"Q10_hist\"].values\n",
    "            out[\"Q10_future_pred\"] = Q10_future\n",
    "            out[\"CF\"] = out[\"Q10_future_pred\"] / out[\"Q10_hist\"]\n",
    "            out.to_csv(os.path.join(args.output_dir, f\"{cont}_future_q10_predictions.csv\"), index=False)\n",
    "        print(\"Prediction complete.\")\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raise SystemExit(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
